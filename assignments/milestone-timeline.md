# Milestones

## RL Agent
Upon reaching this milestone I will have an RL agent that can play a deception-based game program and learn from experience. After training, testing, and verifying that this is a good implementation for the problem, I can use the agent to train itself for the next milestone.

Completion Date: 1/28/22

Deliverables
- Deception-based game program that is programmatically interfaceable
- RL agent that is capable of improving over many iterations of playing the deception-based game
- Documentated evidence of agent's improvement, and results of play against human opponents

## Self-play
This milestone will complete almost all of the technical research and development for the project. At this point, the previously developed RL agent will be used to train itself through lots of gameplay against other agents. The training should converge on an optimal policy for playing the game. 

Completion Date: 3/11/22

Deliverables
- Self-play framework which puts agents against each other to learn from game play
- Self-play agent training initialization parameters
- Documentated evidence of agent's improvement and convergence via self-play

## Documentation
This milestone concludes the documentation processes that I will be continually updating throughout the development, testing, and refinement of this project. At this point, I can analyze the strategies the agent uses to lie, and evaluate the effectiveness and usefulness of this process in this and other problem spaces.

Completion Date: 4/1/22

Deliverables
- Documented results of the converged agent's skill against human opponents
- Documentation of all RL methods and strategies used throughout the development, testing, and refinement process
- Documentation of how the agent learned to use deception and its strategies with using deception

# Timeline

| Task | Start Date | Completion Date |
|-|-|-|
| Design and develop a deception-based game program that is programmatically interfaceable | 10/22/21 | 11/19/21 |
| Research various RL techniques to determine the most applicable strategy for this problem (MC vs TD, etc) | 11/5/21 | 12/17/21 |
| Investigate the need for specific RL agent initialization (instead of starting with a clean slate) | 11/5/21 | 12/17/21 |
| Develop an RL agent that is capable of improving over many iterations of playing the deception-based game | 11/26/21 | 1/28/22 |
| Test the agent for improvement over many games with human opponent(s) | 1/7/22 | 1/28/22 |
| Milestone 1: RL Agent |  | 1/28/22 |
| Research RL methods for self-play | 1/14/22 | 2/11/22 |
| Develop a framework for self-play, where many agents play against each other and all learn from their experiences | 2/4/22 | 3/11/22 |
| Validate the agent's improvement and convergence via self-play | 2/25/22 | 3/11/22 |
| Milestone 2: Self-play |  | 3/11/22 |
| Test the converged agent against a human opponent to evaluate learned skill | 3/11/22 | 3/18/22 |
| Document how the agent learned to use deception over many iterations and investigate the strategies it uses | 1/7/22 | 4/1/22 |
| Milestone 3: Documentation |  | 4/1/22 |

# Effort Matrix
| Task | Effort |
|-|-|
| Design and develop a deception-based game program that is programmatically interfaceable | 10% |
| Research various RL techniques to determine the most applicable strategy for this problem (MC vs TD, etc) | 10% |
| Investigate the need for specific RL agent initialization (instead of starting with a clean slate) | 5% |
| Develop an RL agent that is capable of improving over many iterations of playing the deception-based game | 20% |
| Test the agent for improvement over many games with human opponent(s) | 5% |
| Research RL methods for self-play | 10% |
| Develop a framework for self-play, where many agents play against each other and all learn from their experiences | 20% |
| Validate the agent's improvement and convergence via self-play | 5% |
| Test the converged agent against a human opponent to evaluate learned skill | 5% |
| Document how the agent learned to use deception over many iterations and investigate the strategies it uses | 10% |
